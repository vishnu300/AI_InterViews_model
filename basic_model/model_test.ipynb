{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648be049",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bb889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Sample Job Description ---\n",
      "\n",
      "Extracted Skills/Topics:\n",
      "- 5+ years\n",
      "- a highly motivated senior software engineer\n",
      "- agile\n",
      "- agile/scrum experience\n",
      "- aws\n",
      "- aws cloud services\n",
      "- cloud\n",
      "- communication\n",
      "- containerization\n",
      "- database\n",
      "- database design\n",
      "- design\n",
      "- designing\n",
      "- docker\n",
      "- engineer\n",
      "- excellent problem-solving and communication skills\n",
      "- experience\n",
      "- junior developers\n",
      "- node.js\n",
      "- react\n",
      "- restful apis\n",
      "- scalable web applications\n",
      "- scrum\n",
      "- sql\n",
      "- web\n",
      "- years\n",
      "\n",
      "Top Keywords (General):\n",
      "- seeking\n",
      "- motivated\n",
      "- senior\n",
      "- software\n",
      "- engineer\n",
      "- building\n",
      "- scalable\n",
      "- web\n",
      "\n",
      "Named Entities:\n",
      "- Software Engineer\n",
      "- 5+ years\n",
      "- React\n",
      "- SQL\n",
      "- NoSQL\n",
      "- S3\n",
      "- Lambda\n",
      "- Docker\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# --- Setup spaCy (Run these commands in your terminal if you haven't already) ---\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Load the small English language model for spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run: python -m spacy download en_core_web_sm\")\n",
    "    print(\"Exiting...\")\n",
    "    exit()\n",
    "\n",
    "def analyze_job_description(job_description_text):\n",
    "    \"\"\"\n",
    "    Analyzes a job description to extract potential skills and topics.\n",
    "\n",
    "    Args:\n",
    "        job_description_text (str): The full text of the job description.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'extracted_skills': A list of unique, relevant skills/keywords.\n",
    "            - 'named_entities': A list of identified named entities.\n",
    "            - 'top_keywords': A list of the most frequent relevant words.\n",
    "    \"\"\"\n",
    "    doc = nlp(job_description_text)\n",
    "\n",
    "    # 1. Extract Noun Phrases (often good for skills/technologies)\n",
    "    # Filter out short or irrelevant noun chunks\n",
    "    noun_phrases = [\n",
    "        chunk.text.lower() for chunk in doc.noun_chunks\n",
    "        if len(chunk.text.split()) > 1 or len(chunk.text) > 3 # Filter single letters or very short words\n",
    "    ]\n",
    "\n",
    "    # 2. Identify Named Entities (e.g., organizations, technical terms if custom NER is trained)\n",
    "    # For a general model, this might catch company names, locations, etc.\n",
    "    # For specific skills, you'd need a custom NER model or more advanced techniques.\n",
    "    named_entities = [ent.text for ent in doc.ents]\n",
    "\n",
    "    # 3. Simple Keyword Extraction (based on POS tagging and filtering stopwords)\n",
    "    # Filter out stopwords and punctuation, keep nouns, proper nouns, adjectives, and verbs\n",
    "    keywords = [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha and\n",
    "           token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB']\n",
    "    ]\n",
    "\n",
    "    # Combine and filter for common technical/skill-related terms (a very basic list)\n",
    "    # In a real system, this list would be much more comprehensive or learned.\n",
    "    tech_terms = [\n",
    "        \"python\", \"java\", \"javascript\", \"react\", \"node.js\", \"aws\", \"azure\", \"gcp\",\n",
    "        \"sql\", \"nosql\", \"docker\", \"kubernetes\", \"git\", \"api\", \"rest\", \"graphql\",\n",
    "        \"frontend\", \"backend\", \"fullstack\", \"machine learning\", \"ai\", \"deep learning\",\n",
    "        \"data science\", \"cloud\", \"agile\", \"scrum\", \"devops\", \"testing\", \"security\",\n",
    "        \"linux\", \"windows\", \"mobile\", \"web\", \"database\", \"design\", \"architecture\",\n",
    "        \"communication\", \"teamwork\", \"problem solving\", \"leadership\", \"management\",\n",
    "        \"analyst\", \"engineer\", \"developer\", \"specialist\", \"experience\", \"years\",\n",
    "        \"bachelor's\", \"master's\", \"phd\"\n",
    "    ]\n",
    "\n",
    "    # Filter noun phrases and keywords to find potentially relevant skills\n",
    "    extracted_skills = set()\n",
    "    for phrase in noun_phrases:\n",
    "        if any(term in phrase for term in tech_terms):\n",
    "            extracted_skills.add(phrase)\n",
    "    for keyword in keywords:\n",
    "        if keyword in tech_terms:\n",
    "            extracted_skills.add(keyword)\n",
    "\n",
    "    # Count frequency of keywords for top keywords\n",
    "    keyword_counts = Counter(keywords)\n",
    "    top_keywords = [word for word, count in keyword_counts.most_common(10) if word not in nlp.Defaults.stop_words and word not in [\"years\", \"experience\"]]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"extracted_skills\": sorted(list(extracted_skills)),\n",
    "        \"named_entities\": named_entities,\n",
    "        \"top_keywords\": top_keywords,\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Job Description\n",
    "    sample_jd = \"\"\"\n",
    "    We are seeking a highly motivated Senior Software Engineer with 5+ years of experience\n",
    "    in building scalable web applications. The ideal candidate will have strong expertise\n",
    "    in React and Node.js, with a solid understanding of RESTful APIs and database design\n",
    "    (SQL and NoSQL). Experience with AWS cloud services (EC2, S3, Lambda) and Docker\n",
    "    for containerization is a plus. You will be responsible for designing, developing,\n",
    "    and deploying high-quality code, collaborating with cross-functional teams, and\n",
    "    mentoring junior developers. Excellent problem-solving and communication skills are\n",
    "    essential. Agile/Scrum experience preferred.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- Analyzing Sample Job Description ---\")\n",
    "    analysis_results = analyze_job_description(sample_jd)\n",
    "\n",
    "    print(\"\\nExtracted Skills/Topics:\")\n",
    "    if analysis_results['extracted_skills']:\n",
    "        for skill in analysis_results['extracted_skills']:\n",
    "            print(f\"- {skill}\")\n",
    "    else:\n",
    "        print(\"No specific skills/topics identified based on current filters.\")\n",
    "\n",
    "    print(\"\\nTop Keywords (General):\")\n",
    "    if analysis_results['top_keywords']:\n",
    "        for keyword in analysis_results['top_keywords']:\n",
    "            print(f\"- {keyword}\")\n",
    "    else:\n",
    "        print(\"No top keywords identified.\")\n",
    "\n",
    "    print(\"\\nNamed Entities:\")\n",
    "    if analysis_results['named_entities']:\n",
    "        for entity in analysis_results['named_entities']:\n",
    "            print(f\"- {entity}\")\n",
    "    else:\n",
    "        print(\"No named entities identified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973125f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Dynamic AI Interview Module!\n",
      "This script simulates an AI interviewer.\n",
      "\n",
      "Paste your Job Description below (press Enter twice to finish):\n",
      "\n",
      "--- Starting Job Description Analysis ---\n",
      "\n",
      "Identified Core Topics from JD: \n",
      "No significant topics identified from the job description. Cannot proceed with interview.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "\n",
    "# --- Setup spaCy (Run these commands in your terminal if you haven't already) ---\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Load the small English language model for spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run: python -m spacy download en_core_web_sm\")\n",
    "    print(\"Exiting...\")\n",
    "    exit()\n",
    "\n",
    "def analyze_job_description(job_description_text):\n",
    "    \"\"\"\n",
    "    Analyzes a job description to extract potential skills and topics.\n",
    "    More refined to identify relevant technical and soft skills.\n",
    "    \"\"\"\n",
    "    doc = nlp(job_description_text)\n",
    "\n",
    "    # Define common technical and soft skill keywords for better extraction\n",
    "    # This list can be greatly expanded and refined using a more comprehensive ontology\n",
    "    # or by training a custom classifier.\n",
    "    skill_keywords = [\n",
    "        \"python\", \"java\", \"javascript\", \"react\", \"node.js\", \"aws\", \"azure\", \"gcp\",\n",
    "        \"sql\", \"nosql\", \"docker\", \"kubernetes\", \"git\", \"api\", \"rest\", \"graphql\",\n",
    "        \"frontend\", \"backend\", \"fullstack\", \"machine learning\", \"ai\", \"deep learning\",\n",
    "        \"data science\", \"cloud\", \"agile\", \"scrum\", \"devops\", \"testing\", \"security\",\n",
    "        \"linux\", \"windows\", \"mobile\", \"web\", \"database\", \"design patterns\", \"architecture\",\n",
    "        \"communication\", \"teamwork\", \"problem solving\", \"leadership\", \"management\",\n",
    "        \"analytical\", \"critical thinking\", \"collaboration\", \"mentoring\", \"debugging\",\n",
    "        \"scalability\", \"performance\", \"optimization\", \"microservices\", \"ci/cd\",\n",
    "        \"data structures\", \"algorithms\", \"oop\", \"object-oriented programming\"\n",
    "    ]\n",
    "\n",
    "    extracted_topics = set()\n",
    "    # Extract noun chunks and filter for relevance\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower()\n",
    "        # Check if any skill keyword is a substring of the chunk or vice-versa\n",
    "        if any(keyword in chunk_text for keyword in skill_keywords) or \\\n",
    "           any(chunk_text in keyword for keyword in skill_keywords):\n",
    "            extracted_topics.add(chunk_text)\n",
    "\n",
    "    # Also check individual tokens that are skills\n",
    "    for token in doc:\n",
    "        token_text = token.text.lower()\n",
    "        if token_text in skill_keywords:\n",
    "            extracted_topics.add(token_text)\n",
    "        # Add common phrases like \"problem solving\" if present\n",
    "        if token_text == \"problem\" and token.head.text.lower() == \"solving\":\n",
    "            extracted_topics.add(\"problem solving\")\n",
    "        if token_text == \"data\" and token.head.text.lower() == \"science\":\n",
    "            extracted_topics.add(\"data science\")\n",
    "\n",
    "\n",
    "    # Further filter and refine topics (e.g., remove very generic terms)\n",
    "    final_topics = []\n",
    "    generic_terms = [\"experience\", \"years\", \"role\", \"team\", \"responsibilities\", \"candidate\", \"skills\", \"knowledge\", \"ability\"]\n",
    "    for topic in sorted(list(extracted_topics)):\n",
    "        if topic not in generic_terms and len(topic.split()) <= 4: # Limit phrase length\n",
    "            final_topics.append(topic)\n",
    "\n",
    "    # Prioritize topics that appear more frequently or are explicitly mentioned as requirements\n",
    "    # For simplicity, we'll just return the unique list here.\n",
    "    return final_topics\n",
    "\n",
    "def simulate_llm_question_generation(topic, context=\"\"):\n",
    "    \"\"\"\n",
    "    Simulates an LLM generating an interview question for a given topic.\n",
    "    In a real system, this would be an API call to an LLM.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- LLM Simulation: Generating question for '{topic}' ---\")\n",
    "    if context:\n",
    "        print(f\"Context from previous answers: '{context}'\")\n",
    "        default_question = f\"Can you elaborate more on your experience with {topic}, specifically regarding {context}?\"\n",
    "    else:\n",
    "        default_question = f\"Tell me about your experience with {topic}.\"\n",
    "\n",
    "    # Simulate LLM thinking time\n",
    "    time.sleep(1)\n",
    "    print(\"LLM is thinking...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # In a real scenario, the LLM would return the question.\n",
    "    # Here, we prompt the user for it.\n",
    "    question = input(f\"LLM generated (or suggested): '{default_question}'\\nEnter the actual question for '{topic}': \")\n",
    "    if not question:\n",
    "        question = default_question\n",
    "    return question\n",
    "\n",
    "def simulate_llm_response_analysis(question, candidate_response):\n",
    "    \"\"\"\n",
    "    Simulates an LLM analyzing a candidate's response.\n",
    "    It returns which topics were covered and if any need deeper delving.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- LLM Simulation: Analyzing response for '{question}' ---\")\n",
    "    print(f\"Candidate said: '{candidate_response}'\")\n",
    "\n",
    "    # Simulate LLM thinking time\n",
    "    time.sleep(1)\n",
    "    print(\"LLM is analyzing...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # In a real scenario, the LLM would return structured analysis.\n",
    "    # Here, we prompt the user for it.\n",
    "    covered_topics_input = input(\n",
    "        \"LLM Analysis: Which topics from the JD (comma-separated) were adequately covered in this response? \"\n",
    "        \"(e.g., 'React,Node.js'): \"\n",
    "    )\n",
    "    covered_topics = [t.strip().lower() for t in covered_topics_input.split(',') if t.strip()]\n",
    "\n",
    "    needs_delving_input = input(\n",
    "        \"LLM Analysis: Are there any specific sub-topics or areas from the question/response that need deeper delving? \"\n",
    "        \"(e.g., 'error handling in React', 'database migrations'): \"\n",
    "    )\n",
    "    needs_delving = [t.strip() for t in needs_delving_input.split(',') if t.strip()]\n",
    "\n",
    "    return {\n",
    "        \"covered_topics\": covered_topics,\n",
    "        \"needs_delving\": needs_delving,\n",
    "        \"sentiment\": \"positive\" # Placeholder for actual sentiment analysis\n",
    "    }\n",
    "\n",
    "def run_interview(job_description_text, max_questions=10):\n",
    "    \"\"\"\n",
    "    Orchestrates the dynamic AI interview process.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Job Description Analysis ---\")\n",
    "    initial_topics = analyze_job_description(job_description_text)\n",
    "    print(f\"\\nIdentified Core Topics from JD: {', '.join(initial_topics)}\")\n",
    "\n",
    "    if not initial_topics:\n",
    "        print(\"No significant topics identified from the job description. Cannot proceed with interview.\")\n",
    "        return\n",
    "\n",
    "    # Interview state management\n",
    "    remaining_topics = list(initial_topics)\n",
    "    covered_topics = set()\n",
    "    questions_asked_count = 0\n",
    "    interview_history = [] # Stores (question, response) tuples\n",
    "    topics_to_delve_deeper = [] # Topics identified for follow-up\n",
    "\n",
    "    print(\"\\n--- Beginning AI Interview ---\")\n",
    "    print(\"Type your responses and press Enter.\")\n",
    "    print(\"The AI will simulate question generation and response analysis.\")\n",
    "\n",
    "    while questions_asked_count < max_questions and (remaining_topics or topics_to_delve_deeper):\n",
    "        current_topic = None\n",
    "        question_context = \"\"\n",
    "\n",
    "        if topics_to_delve_deeper:\n",
    "            # Prioritize delving deeper into a specific area\n",
    "            current_topic = topics_to_delve_deeper.pop(0)\n",
    "            question_context = current_topic # Use the sub-topic as context\n",
    "            print(f\"\\nAI is delving deeper into: {current_topic}\")\n",
    "        elif remaining_topics:\n",
    "            # Pick a new, unasked topic\n",
    "            current_topic = remaining_topics.pop(0)\n",
    "            print(f\"\\nAI is asking about a new topic: {current_topic}\")\n",
    "        else:\n",
    "            print(\"\\nAll initial topics covered and no more areas to delve deeper into.\")\n",
    "            break\n",
    "\n",
    "        # Generate question (simulated LLM call)\n",
    "        question = simulate_llm_question_generation(current_topic, question_context)\n",
    "        print(f\"\\nAI: {question}\")\n",
    "\n",
    "        # Get candidate response (simulated microphone input)\n",
    "        candidate_response = input(\"You: \")\n",
    "        if not candidate_response.strip():\n",
    "            print(\"AI: It seems you didn't provide a response. Let's move on or try again.\")\n",
    "            continue\n",
    "\n",
    "        interview_history.append((question, candidate_response))\n",
    "        questions_asked_count += 1\n",
    "\n",
    "        # Analyze response (simulated LLM call)\n",
    "        analysis = simulate_llm_response_analysis(question, candidate_response)\n",
    "\n",
    "        # Update covered topics\n",
    "        for topic in analysis[\"covered_topics\"]:\n",
    "            covered_topics.add(topic)\n",
    "            # Remove from remaining topics if it was there\n",
    "            if topic in remaining_topics:\n",
    "                remaining_topics.remove(topic)\n",
    "\n",
    "        # Add new areas to delve deeper if suggested by analysis\n",
    "        for delve_area in analysis[\"needs_delving\"]:\n",
    "            if delve_area not in topics_to_delve_deeper: # Avoid duplicates\n",
    "                topics_to_delve_deeper.append(delve_area)\n",
    "\n",
    "        print(f\"\\n--- Interview Status ---\")\n",
    "        print(f\"Questions Asked: {questions_asked_count}/{max_questions}\")\n",
    "        print(f\"Covered Topics: {', '.join(covered_topics) if covered_topics else 'None'}\")\n",
    "        print(f\"Remaining Topics: {', '.join(remaining_topics) if remaining_topics else 'None'}\")\n",
    "        print(f\"Areas to Delve Deeper: {', '.join(topics_to_delve_deeper) if topics_to_delve_deeper else 'None'}\")\n",
    "        print(\"------------------------\")\n",
    "\n",
    "    print(\"\\n--- Interview Concluded ---\")\n",
    "    print(\"Interview History:\")\n",
    "    for i, (q, r) in enumerate(interview_history):\n",
    "        print(f\"\\nQ{i+1}: {q}\")\n",
    "        print(f\"A{i+1}: {r}\")\n",
    "\n",
    "    print(\"\\nFinal Summary:\")\n",
    "    print(f\"Total Questions Asked: {questions_asked_count}\")\n",
    "    print(f\"Topics Adequately Covered: {', '.join(covered_topics)}\")\n",
    "    print(f\"Topics Not Fully Explored: {', '.join(remaining_topics)}\")\n",
    "    print(\"In a real system, a detailed report with scores and insights would be generated.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_jd = \"\"\"\n",
    "    We are seeking a highly motivated Senior Software Engineer with 5+ years of experience\n",
    "    in building scalable web applications. The ideal candidate will have strong expertise\n",
    "    in React and Node.js, with a solid understanding of RESTful APIs and database design\n",
    "    (SQL and NoSQL). Experience with AWS cloud services (EC2, S3, Lambda) and Docker\n",
    "    for containerization is a plus. You will be responsible for designing, developing,\n",
    "    and deploying high-quality code, collaborating with cross-functional teams, and\n",
    "    mentoring junior developers. Excellent problem-solving and communication skills are\n",
    "    essential. Agile/Scrum experience preferred.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Welcome to the Dynamic AI Interview Module!\")\n",
    "    print(\"This script simulates an AI interviewer.\")\n",
    "    print(\"\\nPaste your Job Description below (press Enter twice to finish):\")\n",
    "    user_jd_lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if not line:\n",
    "            break\n",
    "        user_jd_lines.append(line)\n",
    "    user_jd = \"\\n\".join(user_jd_lines)\n",
    "\n",
    "    if not user_jd.strip():\n",
    "        print(\"\\nUsing sample job description as no input was provided.\")\n",
    "        user_jd = sample_jd\n",
    "\n",
    "    run_interview(user_jd, max_questions=5) # Limit to 5 questions for a quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16da59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59448e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: AIzaSyCYwpV1Wmn0UIaX1f5C_e_RKqgehro9a9M environment variable not set.\n",
      "Exiting: API Key not configured.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "# import speech_recognition as sr # Example for STT\n",
    "# from flask import Flask, request, jsonify # If building a web API\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Ensure your API key is set as an environment variable named \"GEMINI_API_KEY\"\n",
    "# or \"GOOGLE_API_KEY\" (and adjust the string in os.getenv below accordingly).\n",
    "GEMINI_API_KEY_ENV_VAR_NAME = \"AIzaSyCYwpV1Wmn0UIaX1f5C_e_RKqgehro9a9M\" # Or \"GOOGLE_API_KEY\"\n",
    "GEMINI_API_KEY = os.getenv(GEMINI_API_KEY_ENV_VAR_NAME)\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(f\"Error: {GEMINI_API_KEY_ENV_VAR_NAME} environment variable not set.\")\n",
    "    # exit() # You might want to exit if the key is not found\n",
    "else:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# --- Gemini Model Initialization ---\n",
    "# Using gemini-1.5-pro-latest, which is a highly capable multimodal model.\n",
    "# This can be considered a \"next generation\" or \"version 2\" of the Pro line.\n",
    "# If you specifically need the older 'gemini-pro', you can change it back.\n",
    "try:\n",
    "    text_model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "    # gemini-1.5-pro-latest is multimodal, so a separate vision_model\n",
    "    # using 'gemini-pro-vision' is not strictly necessary if 1.5-pro is used.\n",
    "    # If direct audio/video processing with Gemini 1.5 Pro is intended,\n",
    "    # the input format to generate_content would need to include Parts for those modalities.\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini model: {e}\")\n",
    "    text_model = None # Or handle error appropriately\n",
    "    # exit()\n",
    "\n",
    "class AIInterviewModule:\n",
    "    def __init__(self):\n",
    "        self.job_description = None\n",
    "        self.interview_topics = []\n",
    "        self.interview_questions = []\n",
    "        self.interview_log = [] # To store Qs, As, and Analyses\n",
    "        self.overall_summary = \"\"\n",
    "\n",
    "    def ingest_jd(self, jd_text: str):\n",
    "        if not text_model:\n",
    "            print(\"Error: Gemini text model not initialized.\")\n",
    "            return\n",
    "        self.job_description = jd_text\n",
    "        print(\"Job Description Ingested.\")\n",
    "        self._extract_topics_from_jd()\n",
    "\n",
    "    def _extract_topics_from_jd(self):\n",
    "        if not self.job_description:\n",
    "            print(\"Error: Job description not provided.\")\n",
    "            return\n",
    "        if not text_model:\n",
    "            print(\"Error: Gemini text model not initialized for topic extraction.\")\n",
    "            return\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following job description and extract the top 5-7 key skills,\n",
    "        responsibilities, and required qualifications. List them clearly.\n",
    "        For each, provide a brief explanation of why it's important for the role.\n",
    "\n",
    "        Job Description:\n",
    "        ---\n",
    "        {self.job_description}\n",
    "        ---\n",
    "\n",
    "        Extracted Topics:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = text_model.generate_content(prompt)\n",
    "            # Basic parsing, real implementation might need more robust parsing\n",
    "            self.interview_topics = [topic for topic in response.text.strip().split('\\n') if topic.strip()] # Ensure no empty topics\n",
    "            print(f\"Identified Topics: {self.interview_topics}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting topics from JD with Gemini: {e}\")\n",
    "            self.interview_topics = [] # Fallback or re-attempt\n",
    "\n",
    "    def generate_questions(self, num_questions_per_topic=1):\n",
    "        if not text_model:\n",
    "            print(\"Error: Gemini text model not initialized for question generation.\")\n",
    "            return\n",
    "        if not self.interview_topics:\n",
    "            print(\"Error: No topics identified to generate questions.\")\n",
    "            return\n",
    "\n",
    "        self.interview_questions = []\n",
    "        for topic in self.interview_topics:\n",
    "            if not topic.strip(): continue # Skip empty lines\n",
    "            # Refine topic if it's too verbose from the initial extraction\n",
    "            clean_topic = topic.split('-')[0].strip() # Example of cleaning\n",
    "\n",
    "            # Try to get a brief summary of the job for better context\n",
    "            job_title_or_summary = \"this role\" # Fallback\n",
    "            if self.job_description:\n",
    "                first_line_jd = self.job_description.split('\\n')[0]\n",
    "                job_title_or_summary = f\"a role described as '{first_line_jd}'\"\n",
    "\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are an AI assistant helping generate interview questions for {job_title_or_summary}.\n",
    "            Based on the key skill/responsibility: '{clean_topic}',\n",
    "            generate {num_questions_per_topic} distinct and open-ended interview questions.\n",
    "            If possible, make one behavioral. If the topic sounds technical, try to include one technical question.\n",
    "            If not technical, make another situational or open-ended question to probe experience.\n",
    "\n",
    "            Generated Questions for '{clean_topic}':\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = text_model.generate_content(prompt)\n",
    "                questions_for_topic = response.text.strip().split('\\n')\n",
    "                self.interview_questions.extend([q.lstrip('*- ').strip() for q in questions_for_topic if q.strip()]) # Clean up list markers\n",
    "                print(f\"Generated questions for topic '{clean_topic}': {[q.lstrip('*- ').strip() for q in questions_for_topic if q.strip()]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating questions for topic '{topic}' with Gemini: {e}\")\n",
    "        print(f\"\\nTotal Interview Questions Generated: {len(self.interview_questions)}\")\n",
    "        # print(f\"Questions list: {self.interview_questions}\")\n",
    "\n",
    "\n",
    "    def _transcribe_audio(self, audio_data_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Placeholder for audio transcription.\n",
    "        Gemini 1.5 Pro can process audio directly. If implementing this,\n",
    "        you would pass the audio data as a Part in the generate_content request.\n",
    "        For now, this uses mock transcription.\n",
    "        \"\"\"\n",
    "        print(f\"Simulating transcription for audio from {audio_data_path}...\")\n",
    "        # Example using a hypothetical STT library or Gemini\n",
    "        # r = sr.Recognizer()\n",
    "        # with sr.AudioFile(audio_data_path) as source:\n",
    "        #     audio = r.record(source)\n",
    "        # try:\n",
    "        #     return r.recognize_google(audio) # Uses Google Web Speech API\n",
    "        # except sr.UnknownValueError:\n",
    "        #     return \"Could not understand audio\"\n",
    "        # except sr.RequestError as e:\n",
    "        #     return f\"STT service error; {e}\"\n",
    "        if \"good_answer\" in audio_data_path:\n",
    "            return \"I have extensive experience with project management, leading cross-functional teams to deliver projects on time and within budget. For example, in my previous role at XYZ Corp, I managed a critical software rollout that involved coordinating with engineering, marketing, and sales. We successfully launched the product 10% ahead of schedule and saw a 20% increase in user adoption in the first quarter.\"\n",
    "        elif \"weak_answer\" in audio_data_path:\n",
    "            return \"Um, I know some things about that. I worked on a project once. It was, uh, challenging but we got it done.\"\n",
    "        else:\n",
    "            return \"I'm not sure how to answer that specifically, but I am a fast learner.\"\n",
    "\n",
    "\n",
    "    def analyze_response(self, question: str, answer_transcript: str, relevant_jd_skill: str) -> str:\n",
    "        if not text_model:\n",
    "            print(\"Error: Gemini text model not initialized for response analysis.\")\n",
    "            return \"Analysis failed: Model not initialized.\"\n",
    "        prompt = f\"\"\"\n",
    "        Context:\n",
    "        - Job Description Skill/Requirement Being Assessed: {relevant_jd_skill}\n",
    "        - Interview Question Asked: {question}\n",
    "        - Candidate's Answer: {answer_transcript}\n",
    "\n",
    "        Task:\n",
    "        Evaluate the candidate's answer based on its clarity, relevance to the question,\n",
    "        depth of experience shown, and how well it demonstrates the skill/requirement '{relevant_jd_skill}'.\n",
    "        Provide a brief summary of the answer's strengths and weaknesses regarding this skill.\n",
    "        Give a qualitative rating (e.g., Excellent, Good, Fair, Poor, Needs Improvement).\n",
    "        Be specific in your feedback.\n",
    "\n",
    "        Evaluation:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = text_model.generate_content(prompt)\n",
    "            return response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing response with Gemini: {e}\")\n",
    "            return \"Analysis failed due to an API error.\"\n",
    "\n",
    "    def conduct_interview_session(self):\n",
    "        \"\"\"\n",
    "        Simulates the interview flow.\n",
    "        \"\"\"\n",
    "        if not text_model:\n",
    "            print(\"Error: Gemini text model not initialized. Cannot conduct interview.\")\n",
    "            return\n",
    "        if not self.interview_questions:\n",
    "            print(\"No questions generated. Please generate questions first.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n--- Starting AI Interview ---\")\n",
    "        print(\"Please answer the following questions. Speak clearly.\")\n",
    "\n",
    "        for i, question in enumerate(self.interview_questions):\n",
    "            print(f\"\\nQuestion {i+1}: {question}\")\n",
    "\n",
    "            # Simulate candidate response\n",
    "            simulated_audio_path = \"good_answer.wav\" if i % 2 == 0 else \"weak_answer.wav\"\n",
    "            # In a real app, you might have more varied simulated answers or different audio files\n",
    "            if len(self.interview_questions) > 2 and i == len(self.interview_questions) -1 : # Make last answer different\n",
    "                 simulated_audio_path = \"neutral_answer.wav\"\n",
    "\n",
    "\n",
    "            transcript = self._transcribe_audio(simulated_audio_path)\n",
    "            print(f\"Candidate Transcript: {transcript}\")\n",
    "\n",
    "            # Determine relevant topic for analysis.\n",
    "            # This simplistic mapping assumes questions are generated and asked in topic order.\n",
    "            # A more robust system might store the topic with each question.\n",
    "            relevant_topic_index = i // 1 # Assuming num_questions_per_topic=1\n",
    "            if relevant_topic_index < len(self.interview_topics):\n",
    "                relevant_topic_for_analysis = self.interview_topics[relevant_topic_index]\n",
    "            else:\n",
    "                # Fallback if question count exceeds topics (e.g., if num_questions_per_topic > 1 and not handled perfectly)\n",
    "                relevant_topic_for_analysis = \"General aptitude\"\n",
    "                print(f\"Warning: Could not map question {i+1} to a specific topic. Using fallback.\")\n",
    "\n",
    "\n",
    "            analysis = self.analyze_response(question, transcript, relevant_topic_for_analysis)\n",
    "            print(f\"AI Analysis: {analysis}\")\n",
    "\n",
    "            self.interview_log.append({\n",
    "                \"question\": question,\n",
    "                \"answer_transcript\": transcript,\n",
    "                \"analysis\": analysis,\n",
    "                \"relevant_skill\": relevant_topic_for_analysis\n",
    "            })\n",
    "\n",
    "        print(\"\\n--- Interview Finished ---\")\n",
    "        self._generate_overall_summary()\n",
    "\n",
    "    def _generate_overall_summary(self):\n",
    "        if not text_model:\n",
    "            print(\"Error: Gemini text model not initialized. Cannot generate summary.\")\n",
    "            return\n",
    "        if not self.interview_log:\n",
    "            print(\"No interview log to summarize.\")\n",
    "            return\n",
    "\n",
    "        formatted_log = \"\\n\\n\".join([\n",
    "            f\"Relevant Skill/Topic: {item['relevant_skill']}\\nQuestion: {item['question']}\\nAnswer: {item['answer_transcript']}\\nAI Analysis of Answer: {item['analysis']}\"\n",
    "            for item in self.interview_log\n",
    "        ])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following interview log (which includes questions, candidate answers, AI analyses of those answers, and the relevant skills being assessed),\n",
    "        provide a comprehensive overall summary of the candidate's performance.\n",
    "        Highlight key strengths demonstrated across different skills/topics.\n",
    "        Identify areas for improvement or skills where the candidate showed weakness or lack of depth.\n",
    "        Conclude with an overall qualitative assessment (e.g., Strong Candidate, Promising with Gaps, Not a good fit for these requirements).\n",
    "        Be balanced and base your summary strictly on the provided log.\n",
    "\n",
    "        Interview Log:\n",
    "        ---\n",
    "        {formatted_log}\n",
    "        ---\n",
    "\n",
    "        Overall Summary of Candidate Performance:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = text_model.generate_content(prompt)\n",
    "            print(\"\\n--- Overall Interview Summary (AI Generated) ---\")\n",
    "            print(response.text.strip())\n",
    "            self.overall_summary = response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating overall summary with Gemini: {e}\")\n",
    "            self.overall_summary = \"Summary generation failed due to an API error.\"\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not GEMINI_API_KEY:\n",
    "        print(\"Exiting: API Key not configured.\")\n",
    "    elif not text_model:\n",
    "        print(\"Exiting: Gemini model failed to initialize.\")\n",
    "    else:\n",
    "        module = AIInterviewModule()\n",
    "\n",
    "        sample_jd = \"\"\"\n",
    "        Senior Python Developer\n",
    "\n",
    "        We are seeking an experienced Senior Python Developer to join our dynamic team.\n",
    "        The ideal candidate will have a strong background in developing scalable web applications,\n",
    "        working with RESTful APIs, and database management (SQL and NoSQL).\n",
    "        Responsibilities include designing and implementing new features, writing clean and maintainable code,\n",
    "        collaborating with cross-functional teams, and mentoring junior developers.\n",
    "        Must have 5+ years of Python experience, proficiency with Django/Flask,\n",
    "        and familiarity with cloud platforms (AWS or GCP). Strong problem-solving skills\n",
    "        and excellent communication are essential. Experience with containerization (Docker, Kubernetes)\n",
    "        and CI/CD pipelines is a plus.\n",
    "        \"\"\"\n",
    "        module.ingest_jd(sample_jd)\n",
    "\n",
    "        if module.interview_topics: # Only generate questions if topics were successfully extracted\n",
    "            module.generate_questions(num_questions_per_topic=1) # Generate 1 question per identified main topic\n",
    "        else:\n",
    "            print(\"Skipping question generation as no topics were identified.\")\n",
    "\n",
    "        if module.interview_questions: # Only conduct interview if questions were generated\n",
    "            module.conduct_interview_session()\n",
    "        else:\n",
    "            print(\"Skipping interview session as no questions were generated.\")\n",
    "\n",
    "        # You can access the summary later if needed\n",
    "        # print(\"\\nFinal Stored Summary:\")\n",
    "        # print(module.overall_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a61b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07386e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
